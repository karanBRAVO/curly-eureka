# -*- coding: utf-8 -*-
"""amazon_ml_challenge_2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1huz0a-0Mi4QXk0yPfKgvRVnwywwSGDWT

# Smart Product Pricing Challenge

# Imports
"""

import pandas as pd
import os
import urllib
from pathlib import Path
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import re
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, ViTImageProcessor, ViTModel
from sentence_transformers import SentenceTransformer
from PIL import Image

"""# Folder/File"""

ROOT_FOLDER = "/content"
DATASET_FOLDER = ROOT_FOLDER
DATASET_FILE_PATH = os.path.join(DATASET_FOLDER, "train.csv")
DATASET_IMAGES_FOLDER = os.path.join(ROOT_FOLDER, "training_images")

os.makedirs(DATASET_IMAGES_FOLDER, exist_ok=True)

"""# Load Test Dataset"""

df = pd.read_csv(DATASET_FILE_PATH, nrows=4000)
df.head()

df.columns.to_list()

df.info()

df.shape

df['price'].describe()

df.isnull().sum()

plt.scatter(df.index, df['price'])
plt.xlabel('index')
plt.ylabel('price')
plt.show()

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.hist(df['price'], bins=100)
plt.xlabel('Price')
plt.title('Price Distribution (Original)')

plt.subplot(1, 2, 2)
plt.hist(np.log1p(df['price']), bins=100)
plt.xlabel('Log(Price + 1)')
plt.title('Price Distribution (Log Scale)')
plt.show()

"""# Download Images"""

def download_image(image_link, savefolder):
    if isinstance(image_link, str):
        filename = Path(image_link).name
        image_save_path = os.path.join(savefolder, filename)
        if not os.path.exists(image_save_path):
            try:
                urllib.request.urlretrieve(image_link, image_save_path)
            except Exception as ex:
                print("Warning: Not able to download - {}\n{}".format(image_link, ex))
        else:
            return
    return

if len(os.listdir(DATASET_IMAGES_FOLDER)) == 0:
    for row in tqdm(df.itertuples(), total=len(df), desc="Downloading Images"):
        image_link = row.image_link
        download_image(image_link, DATASET_IMAGES_FOLDER)
else:
    print(f"Images are already downloaded in {DATASET_IMAGES_FOLDER}")

"""# Pre-Processing"""

def pre_process(df):
    df = df.copy()

    # Clean text
    df['catalog_content_clean'] = df['catalog_content'].apply(
        lambda x: re.sub(r'\s+', ' ', str(x).replace('\n', ' ')).lower().strip()
    )

    # Extract the single Value–Unit pair at the end of text
    pattern = r'value:\s*([\d\.]+)\s*unit:\s*([\w\s\.\(\)]+)\s*$'
    df[['value', 'unit']] = df['catalog_content_clean'].apply(
        lambda text: pd.Series(re.search(pattern, text).groups()) if re.search(pattern, text) else pd.Series([None, None])
    )

    # Convert value to float
    df['value'] = df['value'].astype(float)

    return df

new_df = pre_process(df)

# Convert price to log_price
new_df['log_price'] = np.log1p(new_df['price'])

new_df.head()

new_df['log_price'].describe()

new_df.isnull().sum()

new_df.info()

new_df['value'] = new_df['value'].fillna(0)
new_df['unit'] = new_df['unit'].fillna('unknown')

new_df.info()

plt.scatter(new_df.index, new_df['value'])
plt.xlabel('index')
plt.ylabel('value')
plt.show()

"""## Normalization

- Convert every unit to one of 3 standardized metrics:
    - `volume_ml` → for liquids
    - `weight_g` → for solids
    - `count` → for discrete items
- Handle noisy or duplicate spellings like "fl oz", "fl.oz", "fluid ounces", etc.
- Default to `count = 1` for `unknown`/miscellaneous cases.
"""

unit_counts = new_df['unit'].dropna().str.lower().value_counts().head(100)
print("Total Units: ", len(unit_counts))
print(unit_counts)

unit_map = {
    # ---- WEIGHT ----
    "ounce": ("weight_g", 28.3495),
    "oz": ("weight_g", 28.3495),
    "ounces": ("weight_g", 28.3495),
    "pound": ("weight_g", 453.592),
    "pounds": ("weight_g", 453.592),
    "lb": ("weight_g", 453.592),
    "gram": ("weight_g", 1.0),
    "grams": ("weight_g", 1.0),
    "gramm": ("weight_g", 1.0),
    "gr": ("weight_g", 1.0),
    "kg": ("weight_g", 1000.0),

    # ---- VOLUME ----
    "fl oz": ("volume_ml", 29.5735),
    "fl. oz": ("volume_ml", 29.5735),
    "fl.oz": ("volume_ml", 29.5735),
    "fl. oz.": ("volume_ml", 29.5735),
    "fl ounce": ("volume_ml", 29.5735),
    "fluid ounce": ("volume_ml", 29.5735),
    "fluid ounces": ("volume_ml", 29.5735),
    "fluid ounce(s)": ("volume_ml", 29.5735),
    "millilitre": ("volume_ml", 1.0),
    "milliliter": ("volume_ml", 1.0),
    "mililitro": ("volume_ml", 1.0),
    "ml": ("volume_ml", 1.0),
    "ltr": ("volume_ml", 1000.0),
    "liters": ("volume_ml", 1000.0),

    # ---- COUNT / PACK ----
    "count": ("count", 1.0),
    "ct": ("count", 1.0),
    "pack": ("count", 1.0),
    "packs": ("count", 1.0),
    "piece": ("count", 1.0),
    "pieces": ("count", 1.0),
    "unit": ("count", 1.0),
    "units": ("count", 1.0),
    "bag": ("count", 1.0),
    "bottle": ("count", 1.0),
    "bottles": ("count", 1.0),
    "can": ("count", 1.0),
    "jar": ("count", 1.0),
    "box": ("count", 1.0),
    "carton": ("count", 1.0),
    "capsule": ("count", 1.0),
    "pouch": ("count", 1.0),
    "per carton": ("count", 1.0),
    "per package": ("count", 1.0),
    "per box": ("count", 1.0),
    "paper cupcake liners": ("count", 1.0),
    "bucket": ("count", 1.0),
    "sq ft": ("count", 1.0),
    "foot": ("count", 1.0),
    "tea bags": ("count", 1.0),

    # ---- UNKNOWN ----
    "unknown": ("count", 1.0),
    "product_weight": ("weight_g", 1.0),
    "1": ("count", 1.0),
    "8": ("count", 1.0),
    "24": ("count", 1.0),
    "unità": ("count", 1.0),
}

def normalize(row, unit_map):
    val = row['value']
    unit = str(row['unit']).lower().strip()
    if pd.isna(val) or val == 0 or unit == 'nan':
        return pd.Series({'volume_ml': 0, 'weight_g': 0, 'count': 0})

    category, multiplier = unit_map.get(unit, ('count', 1.0))
    volume_ml = weight_g = count = 0

    if category == 'volume_ml':
        volume_ml = val * multiplier
    elif category == 'weight_g':
        weight_g = val * multiplier
    elif category == 'count':
        count = val * multiplier

    return pd.Series({'volume_ml': volume_ml, 'weight_g': weight_g, 'count': count})

new_df[['volume_ml', 'weight_g', 'count']] = new_df.apply(
    normalize, axis=1, unit_map=unit_map
)

new_df.head(10)

new_df.info()

new_df.describe()

new_df.columns.to_list()

new_df.isnull().sum()

"""# Split Dataset"""

train_df, val_df = train_test_split(new_df, test_size=0.2, random_state=42)

print(f"Train samples: {train_df.shape}, Validation samples: {val_df.shape}")

"""# Model Training

┌──────────────────────────┐
            │ SentenceTransformer (text)│ → Text embedding (384-d)
            └──────────────────────────┘
                        │
                        ▼
            ┌──────────────────────────┐
            │ ViT (image)               │ → Image embedding (768-d)
            └──────────────────────────┘
                        │
                        ▼
            ┌──────────────────────────┐
            │ Numeric features (3)      │
            └──────────────────────────┘
                        │
                        ▼
           Concatenate → Dense → ReLU → Dropout → Linear → price

## Loss Function
"""

def smape_loss(y_pred, y_true):
    return torch.mean(
        torch.abs(y_pred - y_true) / ((torch.abs(y_true) + torch.abs(y_pred)) / 2 + 1e-8)
    )

"""## Configurations"""

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
TEXT_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'
IMG_MODEL = 'google/vit-base-patch16-224'
BATCH_SIZE = 16
EPOCHS = 5
LR = 2e-5

print("Device: ", DEVICE)

"""## Dataset for Model"""

class ProductDataset(Dataset):
    def __init__(self, df, img_processor, image_root):
        self.df = df.reset_index(drop=True)
        self.img_processor = img_processor
        self.image_root = image_root

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        text = row['catalog_content_clean']
        image_path = os.path.join(self.image_root, os.path.basename(row['image_link']))

        # Load image
        try:
            image = Image.open(image_path).convert("RGB")
        except:
            image = Image.new("RGB", (224, 224), (255, 255, 255))  # fallback white image

        image_inputs = self.img_processor(image, return_tensors="pt")

        # Numeric features
        num_feats = torch.tensor([
            row.get('volume_ml', 0.0),
            row.get('weight_g', 0.0),
            row.get('count', 0.0)
        ], dtype=torch.float)

        y = torch.tensor(row['log_price'], dtype=torch.float) if 'log_price' in row else 0

        return text, image_inputs, num_feats, y

img_processor = ViTImageProcessor.from_pretrained(IMG_MODEL)

train_ds = ProductDataset(train_df, img_processor, image_root=DATASET_IMAGES_FOLDER)
val_ds = ProductDataset(val_df, img_processor, image_root=DATASET_IMAGES_FOLDER)

train_ds[0]

def collate_fn(batch):
    """Custom collate function to properly batch the data"""
    texts = [item[0] for item in batch]
    image_inputs = [item[1] for item in batch]
    num_feats = torch.stack([item[2] for item in batch])
    targets = torch.stack([item[3] for item in batch])

    # Batch the image inputs properly
    pixel_values = torch.cat([img['pixel_values'] for img in image_inputs], dim=0)

    return texts, pixel_values, num_feats, targets

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)

"""## Model"""

TEXT_EMB_PATH = os.path.join(ROOT_FOLDER, "text_emb.npy")
IMG_EMB_PATH = os.path.join(ROOT_FOLDER, "img_emb.npy")

class MultiModalRegressor(nn.Module):
    def __init__(self, text_model_name, img_model_name):
        super().__init__()
        # Text encoder
        self.text_encoder = SentenceTransformer(text_model_name)
        self.text_emb_dim = self.text_encoder.get_sentence_embedding_dimension()

        # Image encoder
        self.img_encoder = ViTModel.from_pretrained(img_model_name)
        self.img_emb_dim = self.img_encoder.config.hidden_size

        # Numeric features
        self.num_dim = 3

        # Fusion layer
        self.fc = nn.Sequential(
            nn.Linear(self.text_emb_dim + self.img_emb_dim + self.num_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.1),

            nn.Linear(256, 1)
        )

    def forward(self, text_inputs, image_inputs, num_feats):
        # Text embeddings
        text_emb = self.text_encoder.encode(
            text_inputs,
            convert_to_tensor=True,
            device=DEVICE
        )

        # Image embeddings
        # outputs = self.img_encoder(**image_inputs)
        pixel_values = image_inputs.to(DEVICE)
        outputs = self.img_encoder(pixel_values=pixel_values)
        img_emb = outputs.pooler_output

        # Concatenate all features
        fused = torch.cat([text_emb, img_emb, num_feats.to(DEVICE)], dim=1)
        out = self.fc(fused)
        return out.squeeze(1)

model = MultiModalRegressor(TEXT_MODEL, IMG_MODEL).to(DEVICE)
optimizer = torch.optim.AdamW(model.parameters(), lr=LR)

"""## Train"""

MODELS_FOLDER = os.path.join(ROOT_FOLDER, "models")
MODEL_FILE_PATH = os.path.join(MODELS_FOLDER, 'best_model.pth')

os.makedirs(MODELS_FOLDER, exist_ok=True)

def train_model(model, train_loader, val_loader, optimizer):
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for text_inputs, image_inputs, num_feats, y in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
            y = y.to(DEVICE)
            optimizer.zero_grad()
            preds = model(text_inputs, image_inputs, num_feats)
            loss = smape_loss(preds, y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        print(f"Train SMAPE: {total_loss/len(train_loader):.4f}")

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for text_inputs, image_inputs, num_feats, y in val_loader:
                y = y.to(DEVICE)
                preds = model(text_inputs, image_inputs, num_feats)
                loss = smape_loss(preds, y)
                val_loss += loss.item()

        print(f"Val SMAPE: {val_loss/len(val_loader):.4f}")

train_model(model, train_loader, val_loader, optimizer)

torch.save(model.state_dict(), MODEL_FILE_PATH)

"""# Predictions"""

TEST_DATASET_FOLDER = ROOT_FOLDER
TEST_DATASET_FILE_PATH = os.path.join(TEST_DATASET_FOLDER, "test.csv")
TEST_DATASET_IMAGES_FOLDER = os.path.join(ROOT_FOLDER, "testing_images")

os.makedirs(TEST_DATASET_IMAGES_FOLDER, exist_ok=True)

test_df = pd.read_csv(TEST_DATASET_FILE_PATH)

if len(os.listdir(TEST_DATASET_IMAGES_FOLDER)) == 0:
    for row in tqdm(test_df.itertuples(), total=len(test_df), desc="Downloading Test Images"):
        image_link = row.image_link
        download_image(image_link, TEST_DATASET_IMAGES_FOLDER)
else:
    print(f"Images are already downloaded in {TEST_DATASET_IMAGES_FOLDER}")

new_test_df = pre_process(test_df)

new_test_df['value'] = new_test_df['value'].fillna(0)
new_test_df['unit'] = new_test_df['unit'].fillna('unknown')

new_test_df[['volume_ml', 'weight_g', 'count']] = new_test_df.apply(
    normalize, axis=1, unit_map=unit_map
)

new_test_df.shape

new_test_df.head()

new_test_df.info()

"""## Dataset Loader"""

test_ds = ProductDataset(new_test_df, img_processor, TEST_DATASET_IMAGES_FOLDER)
test_ds[0]

def test_collate_fn(batch):
    """Custom collate function to properly batch the data"""
    texts = [item[0] for item in batch]
    image_inputs = [item[1] for item in batch]
    num_feats = torch.stack([item[2] for item in batch])

    # Batch the image inputs properly
    pixel_values = torch.cat([img['pixel_values'] for img in image_inputs], dim=0)

    return texts, pixel_values, num_feats

test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=test_collate_fn)

"""## Predict"""

def predict(model, test_loader):
    model.eval()
    predictions = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Predicting"):
          texts, pixel_values, num_feats = batch
          preds = model(texts, pixel_values, num_feats)
          predictions.extend(preds.cpu().numpy())

    p = np.array(predictions)
    return np.expm1(p)

model.load_state_dict(torch.load(MODEL_FILE_PATH))

predictions = predict(model, test_loader)

"""# Output"""

OUTPUT_FOLDER = os.path.join(ROOT_FOLDER, "output")
OUTPUT_FILE_PATH = os.path.join(OUTPUT_FOLDER, "output.csv")

os.makedirs(OUTPUT_FOLDER, exist_ok=True)

submission = pd.DataFrame({
    'sample_id': new_test_df['sample_id'],
    'price': np.clip(predictions, a_min=0, a_max=None)
})

submission.to_csv(OUTPUT_FILE_PATH, index=False)

print("Completed.")

"""- `College Name`: University Institute of Engineering and Technology, Panjab University (CHD)
- `Team Name`: AI Wizard
- `Members`: Karan Yadav, Nikhil Gupta, Krishna Jain, Amrit Singhal

© 2025
"""

