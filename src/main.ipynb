{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Challenge 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "goal is to create a machine learning model that extracts entity values from images.\n",
    "\n",
    "Output Format: `index, prediction`\n",
    "\n",
    "Evaluation: `F1 Score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Download the images\n",
    "2. Data pre-processing\n",
    "    1. Resize the image\n",
    "    2. normalize the pixel values [0, 1]\n",
    "    3. encode entity_names\n",
    "3. Feature Extraction\n",
    "    1. From images: `Resnet`, `CLIP`, `EfficientNet`, `OCR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs & Outputs to Model\n",
    "\n",
    "Inputs:\n",
    "\n",
    "1. `Image`: The product image, which contains visual information like dimensions, weight, wattage, etc.\n",
    "2. `Entity Name`: A textual descriptor indicating the type of information you need to extract from the image, such as \"item_weight,\" \"voltage,\" or \"width.\"\n",
    "\n",
    "Output:\n",
    "\n",
    "1. `Entity Value`: A string that combines a numerical value and a unit (e.g., \"34 gram\", \"12.5 centimetre\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import time\n",
    "from time import time as timer\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import urllib\n",
    "import pytesseract\n",
    "import easyocr\n",
    "\n",
    "print(\"Imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "entity_unit_map = {\n",
    "    'width': {\n",
    "        'centimetre',\n",
    "        'foot',\n",
    "        'inch',\n",
    "        'metre',\n",
    "        'millimetre',\n",
    "        'yard'\n",
    "    },\n",
    "    'depth': {\n",
    "        'centimetre',\n",
    "        'foot',\n",
    "        'inch',\n",
    "        'metre',\n",
    "        'millimetre',\n",
    "        'yard'\n",
    "    },\n",
    "    'height': {\n",
    "        'centimetre',\n",
    "        'foot',\n",
    "        'inch',\n",
    "        'metre',\n",
    "        'millimetre',\n",
    "        'yard'\n",
    "    },\n",
    "    'item_weight': {\n",
    "        'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'\n",
    "    },\n",
    "    'maximum_weight_recommendation': {\n",
    "        'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'\n",
    "    },\n",
    "    'voltage': {\n",
    "        'kilovolt',\n",
    "        'millivolt',\n",
    "        'volt'\n",
    "    },\n",
    "    'wattage': {\n",
    "        'kilowatt',\n",
    "        'watt'\n",
    "    },\n",
    "    'item_volume': {\n",
    "        'centilitre',\n",
    "        'cubic foot',\n",
    "        'cubic inch',\n",
    "        'cup',\n",
    "        'decilitre',\n",
    "        'fluid ounce',\n",
    "        'gallon',\n",
    "        'imperial gallon',\n",
    "        'litre',\n",
    "        'microlitre',\n",
    "        'millilitre',\n",
    "        'pint',\n",
    "        'quart'\n",
    "    }\n",
    "}\n",
    "\n",
    "allowed_units = {\n",
    "    unit for entity in entity_unit_map for unit in entity_unit_map[entity]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "dataset_dir_path = \"../dataset\"\n",
    "train_file_name = \"train.csv\"\n",
    "test_file_name = \"test.csv\"\n",
    "train_img_dir_path = \"../images/train\"\n",
    "test_img_dir_path = \"../images/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263859, 4)\n"
     ]
    }
   ],
   "source": [
    "# load the train dataset\n",
    "train_dataset_path = os.path.join(dataset_dir_path, train_file_name)\n",
    "df = pd.read_csv(train_dataset_path)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131187, 4)\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "test_dataset_path = os.path.join(dataset_dir_path, test_file_name)\n",
    "test_df = pd.read_csv(test_dataset_path)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>entity_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n",
       "      <td>748919</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>500.0 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n",
       "      <td>916768</td>\n",
       "      <td>item_volume</td>\n",
       "      <td>1.0 cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n",
       "      <td>459516</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.709 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n",
       "      <td>459516</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.709 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n",
       "      <td>731432</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1400 milligram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61QsBSE7jg...</td>\n",
       "      <td>731432</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1400 milligram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://m.media-amazon.com/images/I/81xsq6vf2q...</td>\n",
       "      <td>731432</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1400 milligram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71DiLRHeZd...</td>\n",
       "      <td>731432</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1400 milligram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://m.media-amazon.com/images/I/91Cma3Rzse...</td>\n",
       "      <td>731432</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1400 milligram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71jBLhmTNl...</td>\n",
       "      <td>731432</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1400 milligram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_link  group_id  entity_name  \\\n",
       "0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n",
       "1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n",
       "2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n",
       "3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n",
       "4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n",
       "5  https://m.media-amazon.com/images/I/61QsBSE7jg...    731432  item_weight   \n",
       "6  https://m.media-amazon.com/images/I/81xsq6vf2q...    731432  item_weight   \n",
       "7  https://m.media-amazon.com/images/I/71DiLRHeZd...    731432  item_weight   \n",
       "8  https://m.media-amazon.com/images/I/91Cma3Rzse...    731432  item_weight   \n",
       "9  https://m.media-amazon.com/images/I/71jBLhmTNl...    731432  item_weight   \n",
       "\n",
       "     entity_value  \n",
       "0      500.0 gram  \n",
       "1         1.0 cup  \n",
       "2      0.709 gram  \n",
       "3      0.709 gram  \n",
       "4  1400 milligram  \n",
       "5  1400 milligram  \n",
       "6  1400 milligram  \n",
       "7  1400 milligram  \n",
       "8  1400 milligram  \n",
       "9  1400 milligram  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://m.media-amazon.com/images/I/110EibNycl...</td>\n",
       "      <td>156839</td>\n",
       "      <td>height</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11TU2clswz...</td>\n",
       "      <td>792578</td>\n",
       "      <td>width</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11TU2clswz...</td>\n",
       "      <td>792578</td>\n",
       "      <td>height</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11TU2clswz...</td>\n",
       "      <td>792578</td>\n",
       "      <td>depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11gHj8dhhr...</td>\n",
       "      <td>792578</td>\n",
       "      <td>depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11gHj8dhhr...</td>\n",
       "      <td>792578</td>\n",
       "      <td>height</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11gHj8dhhr...</td>\n",
       "      <td>792578</td>\n",
       "      <td>width</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11lshEUmCr...</td>\n",
       "      <td>156839</td>\n",
       "      <td>height</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>https://m.media-amazon.com/images/I/21+i52HRW4...</td>\n",
       "      <td>478357</td>\n",
       "      <td>width</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>https://m.media-amazon.com/images/I/21-LmSmehZ...</td>\n",
       "      <td>478357</td>\n",
       "      <td>height</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         image_link  group_id  \\\n",
       "0      0  https://m.media-amazon.com/images/I/110EibNycl...    156839   \n",
       "1      1  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
       "2      2  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
       "3      3  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
       "4      4  https://m.media-amazon.com/images/I/11gHj8dhhr...    792578   \n",
       "5      5  https://m.media-amazon.com/images/I/11gHj8dhhr...    792578   \n",
       "6      6  https://m.media-amazon.com/images/I/11gHj8dhhr...    792578   \n",
       "7      7  https://m.media-amazon.com/images/I/11lshEUmCr...    156839   \n",
       "8      8  https://m.media-amazon.com/images/I/21+i52HRW4...    478357   \n",
       "9      9  https://m.media-amazon.com/images/I/21-LmSmehZ...    478357   \n",
       "\n",
       "  entity_name  \n",
       "0      height  \n",
       "1       width  \n",
       "2      height  \n",
       "3       depth  \n",
       "4       depth  \n",
       "5      height  \n",
       "6       width  \n",
       "7      height  \n",
       "8       width  \n",
       "9      height  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def get_image_name_from_url(url: str) -> str:\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "\n",
    "def create_placeholder_image(image_save_path):\n",
    "    try:\n",
    "        placeholder_image = Image.new('RGB', (100, 100), color='black')\n",
    "        placeholder_image.save(image_save_path)\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "\n",
    "def download_image(image_link, save_folder, retries=3, delay=3):\n",
    "    if not isinstance(image_link, str):\n",
    "        return\n",
    "\n",
    "    filename = Path(image_link).name\n",
    "    image_save_path = os.path.join(save_folder, filename)\n",
    "\n",
    "    if os.path.exists(image_save_path):\n",
    "        return\n",
    "\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(image_link, image_save_path)\n",
    "            return\n",
    "        except:\n",
    "            time.sleep(delay)\n",
    "\n",
    "    # Create a black placeholder image for invalid links/images\n",
    "    create_placeholder_image(image_save_path)\n",
    "\n",
    "\n",
    "def download_images(image_links, download_folder, allow_multiprocessing=True):\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    if allow_multiprocessing:\n",
    "        download_image_partial = partial(\n",
    "            download_image, save_folder=download_folder, retries=3, delay=3)\n",
    "\n",
    "        with multiprocessing.Pool(64) as pool:\n",
    "            list(tqdm(pool.imap(download_image_partial,\n",
    "                 image_links), total=len(image_links)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    else:\n",
    "        for image_link in tqdm(image_links, total=len(image_links)):\n",
    "            download_image(\n",
    "                image_link, save_folder=download_folder, retries=3, delay=3)\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263859, 4)\n",
      "(131187, 4)\n"
     ]
    }
   ],
   "source": [
    "# pre-processing\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df.drop_duplicates()\n",
    "\n",
    "print(df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataframe: (211087, 4)\n",
      "Validation Dataframe: (52772, 4)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=.2,\n",
    "    random_state=98,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train Dataframe:\", train_df.shape)\n",
    "print(\"Validation Dataframe:\", val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio = Unique Images / Total Images\n",
      "Train: 205963 / 211087\n",
      "Validation: 52456 / 52772\n",
      "Test: 90666 / 131187\n"
     ]
    }
   ],
   "source": [
    "print(\"Ratio = Unique Images / Total Images\")\n",
    "\n",
    "unique_train_imgs = len(list(set(train_df['image_link'].to_list())))\n",
    "unique_val_imgs = len(list(set(val_df['image_link'].to_list())))\n",
    "unique_test_imgs = len(list(set(test_df['image_link'].to_list())))\n",
    "\n",
    "print(\"Train: \" + str(unique_train_imgs) + \" / \" + str(train_df.shape[0]))\n",
    "print(\"Validation: \" + str(unique_val_imgs) + \" / \" + str(val_df.shape[0]))\n",
    "print(\"Test: \" + str(unique_test_imgs) + \" / \" + str(test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download images\n",
    "train_images = train_df['image_link']\n",
    "val_images = val_df['image_link']\n",
    "test_images = test_df['image_link']\n",
    "\n",
    "train_images_path = os.path.join(train_img_dir_path, \"train\")\n",
    "val_images_path = os.path.join(train_img_dir_path, \"val\")\n",
    "\n",
    "print(\"\\nDownloading train images...\")\n",
    "download_images(train_images, train_images_path, False)\n",
    "\n",
    "print(\"\\nDownloading validation images...\")\n",
    "download_images(val_images, val_images_path, False)\n",
    "\n",
    "print(\"\\nDownloading test images...\")\n",
    "download_images(test_images, test_img_dir_path, False)\n",
    "\n",
    "print(\"\\nTrain images:\", len(os.listdir(train_images_path)))\n",
    "print(\"Validation images:\", len(os.listdir(val_images_path)))\n",
    "print(\"Test images:\", len(os.listdir(test_img_dir_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easyocr reader\n",
    "reader = easyocr.Reader(['en'], gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def display_image(label: str, image, eject: bool):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = tensor_to_image(image)\n",
    "\n",
    "    if eject:\n",
    "        cv2.imshow(label, image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(rgb_image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    # Remove batch dimension and convert to NumPy\n",
    "    image = tensor.squeeze(0).permute(1, 2, 0).detach().numpy()\n",
    "    # Denormalize image (undo mean/std normalization)\n",
    "    image = image * np.array([0.229, 0.224, 0.225]) + \\\n",
    "        np.array([0.485, 0.456, 0.406])\n",
    "    # Convert range from [0, 1] to [0, 255]\n",
    "    image = np.clip(image * 255.0, 0, 255).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "def extract_text(image, b: bool):\n",
    "    text = \"\"\n",
    "    if b:\n",
    "        text = reader.readtext(image, detail=0, paragraph=True)\n",
    "    else:\n",
    "        pil_image = Image.fromarray(image)\n",
    "        text = pytesseract.image_to_string(pil_image)\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_image_for_clear_text(image_path):\n",
    "    # Step 1: Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(\"Image not found or invalid image path\")\n",
    "\n",
    "    # Step 2: Convert to Grayscale\n",
    "    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Step 3: Invert the image\n",
    "    inverted_image = cv2.bitwise_not(grayscale_image)\n",
    "    # return inverted_image\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    contrast_enhanced = clahe.apply(inverted_image)\n",
    "\n",
    "    return contrast_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_pil(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('L')\n",
    "    img = img.filter(ImageFilter.SHARPEN)\n",
    "    img = ImageEnhance.Contrast(img).enhance(1.5)\n",
    "    img_array = np.array(img)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = train_images.to_list()\n",
    "total_images = 2\n",
    "\n",
    "for i in range(total_images):\n",
    "    img_name = get_image_name_from_url(training_images[i])\n",
    "    img_path = os.path.normpath(os.path.join(train_images_path, img_name))\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"[!] {img_path} Not found\")\n",
    "    else:\n",
    "        print(img_path)\n",
    "        img = cv2.imread(img_path)\n",
    "        display_image(\"Image\", img, False)\n",
    "\n",
    "        pil_img = preprocess_image_pil(img_path)\n",
    "        display_image(\"Image\", pil_img, False)\n",
    "\n",
    "        img_text = extract_text(pil_img, True)\n",
    "        print(img_text)\n",
    "\n",
    "        # processed_img = process_image_for_clear_text(img_path)\n",
    "        # display_image(\"Image\", processed_img, False)\n",
    "\n",
    "        # img_text = extract_text(processed_img, True)\n",
    "        # print(img_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=200.0,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path = \"..\\\\images\\\\test\\\\110EibNyclL.jpg\"\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "display_image(\"Image\", image, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process(img_path):\n",
    "    # Read the image\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    # Use adaptive thresholding to enhance text\n",
    "    enhanced_text = cv2.adaptiveThreshold(blurred, 255,\n",
    "                                          cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                          cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Sharpen the image to make text crisper\n",
    "    kernel_sharpen = np.array([[-1, -1, -1],\n",
    "                               [-1,  9, -1],\n",
    "                               [-1, -1, -1]])\n",
    "    sharpened = cv2.filter2D(enhanced_text, -1, kernel_sharpen)\n",
    "\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    # To make the text thinner (erosion)\n",
    "    # eroded = cv2.erode(sharpened, kernel, iterations=1)\n",
    "    dilated = cv2.dilate(sharpened, kernel, iterations=1)\n",
    "\n",
    "    return dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_image = _process(image_path)\n",
    "display_image(\"processed_image\", processed_image, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message and prompt\n",
    "prompt = \"\"\"\n",
    "Please analyze the image and provide a detailed list of all measurable properties and specifications shown, including but not limited to:\n",
    "\n",
    "{\n",
    "    length\n",
    "    width\n",
    "    height\n",
    "    depth\n",
    "    weight\n",
    "    volume\n",
    "    wattage\n",
    "    maximum_weight_recommendation\n",
    "    voltage\n",
    "}\n",
    "Any numerical values or measurements displayed\n",
    "Any text or labels visible\n",
    "\n",
    "Please list each property with its corresponding value and unit of measurement where applicable. If any standard properties like voltage or wattage are not depicted, simply note that they are not shown in the image. Provide the output in a JSON-like format with key-value pairs. Use the exact keys specified below. Include the unit in the value string. If a specification is not visible or applicable, use 'N/A' as the value.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_path,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
