{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Challenge 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "goal is to create a machine learning model that extracts entity values from images.\n",
    "\n",
    "Output Format: `index, prediction`\n",
    "\n",
    "Evaluation: `F1 Score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Download the images\n",
    "2. Data pre-processing\n",
    "    1. Resize the image\n",
    "    2. normalize the pixel values [0, 1]\n",
    "    3. encode entity_names\n",
    "3. Feature Extraction\n",
    "    1. From images: `Resnet`, `CLIP`, `EfficientNet`, `OCR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs & Outputs to Model\n",
    "\n",
    "Inputs:\n",
    "\n",
    "1. `Image`: The product image, which contains visual information like dimensions, weight, wattage, etc.\n",
    "2. `Entity Name`: A textual descriptor indicating the type of information you need to extract from the image, such as \"item_weight,\" \"voltage,\" or \"width.\"\n",
    "\n",
    "Output:\n",
    "\n",
    "1. `Entity Value`: A string that combines a numerical value and a unit (e.g., \"34 gram\", \"12.5 centimetre\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import time\n",
    "from time import time as timer\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import urllib\n",
    "import pytesseract\n",
    "import easyocr\n",
    "\n",
    "print(\"Imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "entity_unit_map = {\n",
    "    'width': {\n",
    "        'centimetre',\n",
    "        'foot',\n",
    "        'inch',\n",
    "        'metre',\n",
    "        'millimetre',\n",
    "        'yard'\n",
    "    },\n",
    "    'depth': {\n",
    "        'centimetre',\n",
    "        'foot',\n",
    "        'inch',\n",
    "        'metre',\n",
    "        'millimetre',\n",
    "        'yard'\n",
    "    },\n",
    "    'height': {\n",
    "        'centimetre',\n",
    "        'foot',\n",
    "        'inch',\n",
    "        'metre',\n",
    "        'millimetre',\n",
    "        'yard'\n",
    "    },\n",
    "    'item_weight': {\n",
    "        'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'\n",
    "    },\n",
    "    'maximum_weight_recommendation': {\n",
    "        'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'\n",
    "    },\n",
    "    'voltage': {\n",
    "        'kilovolt',\n",
    "        'millivolt',\n",
    "        'volt'\n",
    "    },\n",
    "    'wattage': {\n",
    "        'kilowatt',\n",
    "        'watt'\n",
    "    },\n",
    "    'item_volume': {\n",
    "        'centilitre',\n",
    "        'cubic foot',\n",
    "        'cubic inch',\n",
    "        'cup',\n",
    "        'decilitre',\n",
    "        'fluid ounce',\n",
    "        'gallon',\n",
    "        'imperial gallon',\n",
    "        'litre',\n",
    "        'microlitre',\n",
    "        'millilitre',\n",
    "        'pint',\n",
    "        'quart'\n",
    "    }\n",
    "}\n",
    "\n",
    "allowed_units = {\n",
    "    unit for entity in entity_unit_map for unit in entity_unit_map[entity]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "dataset_dir_path = \"../dataset\"\n",
    "train_file_name = \"train.csv\"\n",
    "test_file_name = \"test.csv\"\n",
    "train_img_dir_path = \"../images/train\"\n",
    "test_img_dir_path = \"../images/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train dataset\n",
    "train_dataset_path = os.path.join(dataset_dir_path, train_file_name)\n",
    "df = pd.read_csv(train_dataset_path)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test dataset\n",
    "test_dataset_path = os.path.join(dataset_dir_path, test_file_name)\n",
    "test_df = pd.read_csv(test_dataset_path)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def get_image_name_from_url(url: str) -> str:\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "\n",
    "def create_placeholder_image(image_save_path):\n",
    "    try:\n",
    "        placeholder_image = Image.new('RGB', (100, 100), color='black')\n",
    "        placeholder_image.save(image_save_path)\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "\n",
    "def download_image(image_link, save_folder, retries=3, delay=3):\n",
    "    if not isinstance(image_link, str):\n",
    "        return\n",
    "\n",
    "    filename = Path(image_link).name\n",
    "    image_save_path = os.path.join(save_folder, filename)\n",
    "\n",
    "    if os.path.exists(image_save_path):\n",
    "        return\n",
    "\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(image_link, image_save_path)\n",
    "            return\n",
    "        except:\n",
    "            time.sleep(delay)\n",
    "\n",
    "    # Create a black placeholder image for invalid links/images\n",
    "    create_placeholder_image(image_save_path)\n",
    "\n",
    "\n",
    "def download_images(image_links, download_folder, allow_multiprocessing=True):\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    if allow_multiprocessing:\n",
    "        download_image_partial = partial(\n",
    "            download_image, save_folder=download_folder, retries=3, delay=3)\n",
    "\n",
    "        with multiprocessing.Pool(64) as pool:\n",
    "            list(tqdm(pool.imap(download_image_partial,\n",
    "                 image_links), total=len(image_links)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    else:\n",
    "        for image_link in tqdm(image_links, total=len(image_links)):\n",
    "            download_image(\n",
    "                image_link, save_folder=download_folder, retries=3, delay=3)\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df.drop_duplicates()\n",
    "\n",
    "print(df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=.2,\n",
    "    random_state=98,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train Dataframe:\", train_df.shape)\n",
    "print(\"Validation Dataframe:\", val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ratio = Unique Images / Total Images\")\n",
    "\n",
    "unique_train_imgs = len(list(set(train_df['image_link'].to_list())))\n",
    "unique_val_imgs = len(list(set(val_df['image_link'].to_list())))\n",
    "unique_test_imgs = len(list(set(test_df['image_link'].to_list())))\n",
    "\n",
    "print(\"Train: \" + str(unique_train_imgs) + \" / \" + str(train_df.shape[0]))\n",
    "print(\"Validation: \" + str(unique_val_imgs) + \" / \" + str(val_df.shape[0]))\n",
    "print(\"Test: \" + str(unique_test_imgs) + \" / \" + str(test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 78280/131187 [3:09:05<2:17:56,  6.39it/s]  "
     ]
    }
   ],
   "source": [
    "# download images\n",
    "train_images = train_df['image_link']\n",
    "val_images = val_df['image_link']\n",
    "test_images = test_df['image_link']\n",
    "\n",
    "train_images_path = os.path.join(train_img_dir_path, \"train\")\n",
    "val_images_path = os.path.join(train_img_dir_path, \"val\")\n",
    "\n",
    "print(\"\\nDownloading train images...\")\n",
    "download_images(train_images, train_images_path, False)\n",
    "\n",
    "print(\"\\nDownloading validation images...\")\n",
    "download_images(val_images, val_images_path, False)\n",
    "\n",
    "print(\"\\nDownloading test images...\")\n",
    "download_images(test_images, test_img_dir_path, False)\n",
    "\n",
    "print(\"\\nTrain images:\", len(os.listdir(train_images_path)))\n",
    "print(\"Validation images:\", len(os.listdir(val_images_path)))\n",
    "print(\"Test images:\", len(os.listdir(test_img_dir_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easyocr reader\n",
    "reader = easyocr.Reader(['en'], gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def display_image(label: str, image, eject: bool):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = tensor_to_image(image)\n",
    "\n",
    "    if eject:\n",
    "        cv2.imshow(label, image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(rgb_image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    # Remove batch dimension and convert to NumPy\n",
    "    image = tensor.squeeze(0).permute(1, 2, 0).detach().numpy()\n",
    "    # Denormalize image (undo mean/std normalization)\n",
    "    image = image * np.array([0.229, 0.224, 0.225]) + \\\n",
    "        np.array([0.485, 0.456, 0.406])\n",
    "    # Convert range from [0, 1] to [0, 255]\n",
    "    image = np.clip(image * 255.0, 0, 255).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "def extract_text(image, b: bool):\n",
    "    text = \"\"\n",
    "    if b:\n",
    "        text = reader.readtext(image, detail=0, paragraph=True)\n",
    "    else:\n",
    "        pil_image = Image.fromarray(image)\n",
    "        text = pytesseract.image_to_string(pil_image)\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_image_for_clear_text(image_path):\n",
    "    # Step 1: Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(\"Image not found or invalid image path\")\n",
    "\n",
    "    # Step 2: Convert to Grayscale\n",
    "    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Step 3: Invert the image\n",
    "    inverted_image = cv2.bitwise_not(grayscale_image)\n",
    "    # return inverted_image\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    contrast_enhanced = clahe.apply(inverted_image)\n",
    "\n",
    "    return contrast_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_pil(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('L')\n",
    "    img = img.filter(ImageFilter.SHARPEN)\n",
    "    img = ImageEnhance.Contrast(img).enhance(1.5)\n",
    "    img_array = np.array(img)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = train_images.to_list()\n",
    "total_images = 2\n",
    "\n",
    "for i in range(total_images):\n",
    "    img_name = get_image_name_from_url(training_images[i])\n",
    "    img_path = os.path.normpath(os.path.join(train_images_path, img_name))\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"[!] {img_path} Not found\")\n",
    "    else:\n",
    "        print(img_path)\n",
    "        img = cv2.imread(img_path)\n",
    "        display_image(\"Image\", img, False)\n",
    "\n",
    "        pil_img = preprocess_image_pil(img_path)\n",
    "        display_image(\"Image\", pil_img, False)\n",
    "\n",
    "        img_text = extract_text(pil_img, True)\n",
    "        print(img_text)\n",
    "\n",
    "        # processed_img = process_image_for_clear_text(img_path)\n",
    "        # display_image(\"Image\", processed_img, False)\n",
    "\n",
    "        # img_text = extract_text(processed_img, True)\n",
    "        # print(img_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default: Load the model on the available device(s)\n",
    "model = transformers.Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default processer\n",
    "processor = transformers.AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"..\\\\images\\\\train\\\\train\\\\81nhXKiRWLL.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message and prompt\n",
    "prompt = \"\"\"\n",
    "Analyze this image and extract the following specifications. Provide the output in a JSON-like format with key-value pairs. Use the exact keys specified below. Include the unit in the value string. If a specification is not visible or applicable, use 'N/A' as the value.\n",
    "\n",
    "{\n",
    "  weight:\n",
    "  height:\n",
    "  length:\n",
    "  width:\n",
    "  depth:\n",
    "  volume:\n",
    "  voltage:\n",
    "  wattage:\n",
    "}\n",
    "\n",
    "Only include specifications directly observable in the image. give output in JSON-like structure.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_path,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
